{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sys\n",
    "import torch\n",
    "import tqdm\n",
    "from scipy.spatial.distance import cosine\n",
    "import transformers as tfm\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list = ['bert','bertLarge','gpt2', 'roberta', 'mpnet']\n",
    "models_output_size = {'bert':768,'bertLarge':1024,'gpt2':768,'roberta':768,'mpnet':768}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers_func = [tfm.BertTokenizer,tfm.BertTokenizer,tfm.GPT2Tokenizer,tfm.RobertaTokenizer,tfm.MPNetTokenizer]\n",
    "models_func = [tfm.BertModel,tfm.BertModel,tfm.GPT2Model,tfm.RobertaModel,tfm.MPNetModel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/bastien/Downloads/LanguageModels/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/bastien/Downloads/LanguageModels/bert were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at /home/bastien/Downloads/LanguageModels/bertLarge were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at /home/bastien/Downloads/LanguageModels/roberta were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at /home/bastien/Downloads/LanguageModels/mpnet were not used when initializing MPNetModel: ['lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing MPNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MPNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MPNetModel were not initialized from the model checkpoint at /home/bastien/Downloads/LanguageModels/mpnet and are newly initialized: ['mpnet.pooler.dense.bias', 'mpnet.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizers = {models_list[i] : tokenizers_func[i].from_pretrained(path+models_list[i]) for i in range(len(models_list))}\n",
    "models = {models_list[i] : models_func[i].from_pretrained(path+models_list[i]) for i in range(len(models_list))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = ['I come from', 'He lives in', 'She moved to']\n",
    "short_ctxts = ['come','lives','moved']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = pd.read_csv('./csv/worldcitiespop.csv',header=0,dtype={'AccentCity':'str', 'Region':'object'}).dropna().drop('Region',axis=1)\n",
    "cities = cities[cities.Population>100000].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>City</th>\n",
       "      <th>AccentCity</th>\n",
       "      <th>Population</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ae</td>\n",
       "      <td>abu dhabi</td>\n",
       "      <td>Abu Dhabi</td>\n",
       "      <td>603687.0</td>\n",
       "      <td>24.466667</td>\n",
       "      <td>54.366667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ae</td>\n",
       "      <td>dubai</td>\n",
       "      <td>Dubai</td>\n",
       "      <td>1137376.0</td>\n",
       "      <td>25.258172</td>\n",
       "      <td>55.304717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ae</td>\n",
       "      <td>sharjah</td>\n",
       "      <td>Sharjah</td>\n",
       "      <td>543942.0</td>\n",
       "      <td>25.357310</td>\n",
       "      <td>55.403304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>af</td>\n",
       "      <td>baglan</td>\n",
       "      <td>Baglan</td>\n",
       "      <td>108481.0</td>\n",
       "      <td>36.130684</td>\n",
       "      <td>68.708286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>af</td>\n",
       "      <td>gardez</td>\n",
       "      <td>Gardez</td>\n",
       "      <td>103732.0</td>\n",
       "      <td>33.597439</td>\n",
       "      <td>69.225922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3522</th>\n",
       "      <td>zw</td>\n",
       "      <td>gweru</td>\n",
       "      <td>Gweru</td>\n",
       "      <td>201879.0</td>\n",
       "      <td>-19.450000</td>\n",
       "      <td>29.816667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3523</th>\n",
       "      <td>zw</td>\n",
       "      <td>harare</td>\n",
       "      <td>Harare</td>\n",
       "      <td>2213701.0</td>\n",
       "      <td>-17.817778</td>\n",
       "      <td>31.044722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3524</th>\n",
       "      <td>zw</td>\n",
       "      <td>kadoma</td>\n",
       "      <td>Kadoma</td>\n",
       "      <td>100276.0</td>\n",
       "      <td>-18.350000</td>\n",
       "      <td>29.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3525</th>\n",
       "      <td>zw</td>\n",
       "      <td>kwekwe</td>\n",
       "      <td>Kwekwe</td>\n",
       "      <td>116332.0</td>\n",
       "      <td>-18.916667</td>\n",
       "      <td>29.816667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3526</th>\n",
       "      <td>zw</td>\n",
       "      <td>mutare</td>\n",
       "      <td>Mutare</td>\n",
       "      <td>253449.0</td>\n",
       "      <td>-18.966667</td>\n",
       "      <td>32.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3527 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Country       City AccentCity  Population   Latitude  Longitude\n",
       "0         ae  abu dhabi  Abu Dhabi    603687.0  24.466667  54.366667\n",
       "1         ae      dubai      Dubai   1137376.0  25.258172  55.304717\n",
       "2         ae    sharjah    Sharjah    543942.0  25.357310  55.403304\n",
       "3         af     baglan     Baglan    108481.0  36.130684  68.708286\n",
       "4         af     gardez     Gardez    103732.0  33.597439  69.225922\n",
       "...      ...        ...        ...         ...        ...        ...\n",
       "3522      zw      gweru      Gweru    201879.0 -19.450000  29.816667\n",
       "3523      zw     harare     Harare   2213701.0 -17.817778  31.044722\n",
       "3524      zw     kadoma     Kadoma    100276.0 -18.350000  29.916667\n",
       "3525      zw     kwekwe     Kwekwe    116332.0 -18.916667  29.816667\n",
       "3526      zw     mutare     Mutare    253449.0 -18.966667  32.666667\n",
       "\n",
       "[3527 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CountryName</th>\n",
       "      <th>CapitalName</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Somaliland</td>\n",
       "      <td>Hargeisa</td>\n",
       "      <td>9.550000</td>\n",
       "      <td>44.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>South Georgia and South Sandwich Islands</td>\n",
       "      <td>King Edward Point</td>\n",
       "      <td>-54.283333</td>\n",
       "      <td>-36.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>French Southern and Antarctic Lands</td>\n",
       "      <td>Port-aux-Français</td>\n",
       "      <td>-49.350000</td>\n",
       "      <td>70.216667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Palestine</td>\n",
       "      <td>Jerusalem</td>\n",
       "      <td>31.766667</td>\n",
       "      <td>35.233333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aland Islands</td>\n",
       "      <td>Mariehamn</td>\n",
       "      <td>60.116667</td>\n",
       "      <td>19.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>Harare</td>\n",
       "      <td>-17.816667</td>\n",
       "      <td>31.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>Northern Cyprus</td>\n",
       "      <td>North Nicosia</td>\n",
       "      <td>35.183333</td>\n",
       "      <td>33.366667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>22.302711</td>\n",
       "      <td>114.177216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>British Indian Ocean Territory</td>\n",
       "      <td>Diego Garcia</td>\n",
       "      <td>-7.300000</td>\n",
       "      <td>72.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>Macau</td>\n",
       "      <td>Macau</td>\n",
       "      <td>22.210928</td>\n",
       "      <td>113.552971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>242 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  CountryName        CapitalName   Latitude  \\\n",
       "0                                  Somaliland           Hargeisa   9.550000   \n",
       "1    South Georgia and South Sandwich Islands  King Edward Point -54.283333   \n",
       "2         French Southern and Antarctic Lands  Port-aux-Français -49.350000   \n",
       "3                                   Palestine          Jerusalem  31.766667   \n",
       "4                               Aland Islands          Mariehamn  60.116667   \n",
       "..                                        ...                ...        ...   \n",
       "237                                  Zimbabwe             Harare -17.816667   \n",
       "238                           Northern Cyprus      North Nicosia  35.183333   \n",
       "239                                 Hong Kong          Hong Kong  22.302711   \n",
       "240            British Indian Ocean Territory       Diego Garcia  -7.300000   \n",
       "241                                     Macau              Macau  22.210928   \n",
       "\n",
       "      Longitude  \n",
       "0     44.050000  \n",
       "1    -36.500000  \n",
       "2     70.216667  \n",
       "3     35.233333  \n",
       "4     19.900000  \n",
       "..          ...  \n",
       "237   31.033333  \n",
       "238   33.366667  \n",
       "239  114.177216  \n",
       "240   72.400000  \n",
       "241  113.552971  \n",
       "\n",
       "[242 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capitals = pd.read_csv('./csv/country-capitals.csv').drop(['ContinentName','CountryCode'], axis=1)\n",
    "capitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Code</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Andorra</td>\n",
       "      <td>AD</td>\n",
       "      <td>42.5000</td>\n",
       "      <td>1.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>AE</td>\n",
       "      <td>24.0000</td>\n",
       "      <td>54.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AF</td>\n",
       "      <td>33.0000</td>\n",
       "      <td>65.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Antigua and Barbuda</td>\n",
       "      <td>AG</td>\n",
       "      <td>17.0500</td>\n",
       "      <td>-61.8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anguilla</td>\n",
       "      <td>AI</td>\n",
       "      <td>18.2500</td>\n",
       "      <td>-63.1667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>Yemen</td>\n",
       "      <td>YE</td>\n",
       "      <td>15.0000</td>\n",
       "      <td>48.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>Mayotte</td>\n",
       "      <td>YT</td>\n",
       "      <td>-12.8333</td>\n",
       "      <td>45.1667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>South Africa</td>\n",
       "      <td>ZA</td>\n",
       "      <td>-29.0000</td>\n",
       "      <td>24.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>Zambia</td>\n",
       "      <td>ZM</td>\n",
       "      <td>-15.0000</td>\n",
       "      <td>30.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>ZW</td>\n",
       "      <td>-20.0000</td>\n",
       "      <td>30.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>249 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Name Code  Latitude  Longitude\n",
       "0                 Andorra   AD   42.5000     1.5000\n",
       "1    United Arab Emirates   AE   24.0000    54.0000\n",
       "2             Afghanistan   AF   33.0000    65.0000\n",
       "3     Antigua and Barbuda   AG   17.0500   -61.8000\n",
       "4                Anguilla   AI   18.2500   -63.1667\n",
       "..                    ...  ...       ...        ...\n",
       "244                 Yemen   YE   15.0000    48.0000\n",
       "245               Mayotte   YT  -12.8333    45.1667\n",
       "246          South Africa   ZA  -29.0000    24.0000\n",
       "247                Zambia   ZM  -15.0000    30.0000\n",
       "248              Zimbabwe   ZW  -20.0000    30.0000\n",
       "\n",
       "[249 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries = pd.read_csv('./csv/countries.csv',keep_default_na=False)\n",
    "countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert'\n",
    "ctx = contexts[0]\n",
    "token = tokenizers[model_name]\n",
    "model = models[model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry = 'Paris'\n",
    "inputs = token(ctx+\" \"+entry,return_tensors=\"pt\")\n",
    "outputs = model(**inputs,output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 6.6166e-01,  1.5474e-01,  1.1787e-01,  ..., -6.8726e-02,\n",
       "           3.0570e-01, -2.5763e-02],\n",
       "         [ 3.9590e-01,  6.6684e-02,  4.5810e-01,  ...,  4.2802e-01,\n",
       "           3.7462e-02,  1.2796e-02],\n",
       "         [ 4.4277e-01,  1.5790e-01, -5.7216e-02,  ...,  6.8028e-01,\n",
       "          -2.0171e-01, -1.1367e-01],\n",
       "         [ 1.2365e-01, -1.7644e-02,  5.1875e-01,  ..., -8.7626e-02,\n",
       "           2.6407e-01, -1.8693e-01],\n",
       "         [ 1.6168e-01,  1.0686e-01, -1.4996e-02,  ..., -3.0191e-01,\n",
       "           3.8160e-01,  3.8241e-04],\n",
       "         [ 1.0472e+00,  4.1524e-02, -5.4551e-01,  ...,  4.7855e-01,\n",
       "           8.1386e-01, -1.5856e-02]]], grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "ind = 100\n",
    "ctx=contexts[i]\n",
    "\n",
    "for entry in cities.AccentCity :\n",
    "    inputs = token(ctx+\" \"+entry,return_tensors=\"pt\")\n",
    "    \n",
    "    #If the entry is split in multiple tokens, we need to aggregate the tensors\n",
    "    entry_tokens = token([entry],is_split_into_words=True,add_special_tokens=False)['input_ids']\n",
    "\n",
    "    expected_length = len(token([entry],is_split_into_words=True,add_special_tokens=False)['input_ids']) #number of tokens for the entry\n",
    "    last_ctx_token = token(ctx,add_special_tokens=False)['input_ids'][-1] #last token of the context string\n",
    "    start_index = inputs['input_ids'][0].tolist().index(last_ctx_token)+1 #first index of the entry's tensors\n",
    "    if inputs['input_ids'][0][start_index:start_index+expected_length].tolist()!= entry_tokens:\n",
    "        print(entry_tokens)\n",
    "        print(inputs['input_ids'][0][start_index:start_index+expected_length])\n",
    "        raise ValueError()\n",
    "\n",
    "\n",
    "#entry_embedding = outputs.last_hidden_state[0][start_index:start_index+expected_length].mean(axis=0)\n",
    "\n",
    "#entry_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last layer embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bert : come: 100%|██████████| 3.53k/3.53k [05:13<00:00, 11.2it/s]\n",
      "bert : lives: 100%|██████████| 3.53k/3.53k [04:32<00:00, 12.9it/s]\n",
      "bert : moved: 100%|██████████| 3.53k/3.53k [04:16<00:00, 13.7it/s]\n",
      "bertLarge : come: 100%|██████████| 3.53k/3.53k [13:27<00:00, 4.37it/s]  \n",
      "bertLarge : lives: 100%|██████████| 3.53k/3.53k [13:33<00:00, 4.34it/s]\n",
      "bertLarge : moved: 100%|██████████| 3.53k/3.53k [13:38<00:00, 4.31it/s]\n",
      "gpt2 : come: 100%|██████████| 3.53k/3.53k [04:28<00:00, 13.1it/s] \n",
      "gpt2 : lives: 100%|██████████| 3.53k/3.53k [04:18<00:00, 13.6it/s]\n",
      "gpt2 : moved: 100%|██████████| 3.53k/3.53k [04:04<00:00, 14.4it/s]\n",
      "roberta : come: 100%|██████████| 3.53k/3.53k [04:03<00:00, 14.5it/s] \n",
      "roberta : lives: 100%|██████████| 3.53k/3.53k [03:48<00:00, 15.4it/s]\n",
      "roberta : moved: 100%|██████████| 3.53k/3.53k [03:54<00:00, 15.0it/s]\n",
      "mpnet : come: 100%|██████████| 3.53k/3.53k [04:13<00:00, 13.9it/s]\n",
      "mpnet : lives: 100%|██████████| 3.53k/3.53k [04:06<00:00, 14.3it/s]\n",
      "mpnet : moved: 100%|██████████| 3.53k/3.53k [04:11<00:00, 14.0it/s]\n"
     ]
    }
   ],
   "source": [
    "# Cities\n",
    "series = cities.AccentCity\n",
    "for model_name in models_list:\n",
    "    token = tokenizers[model_name]\n",
    "    model = models[model_name]\n",
    "    size = models_output_size[model_name]\n",
    "    for i, ctx in enumerate(contexts):\n",
    "        buffer_arr = np.empty((len(series),size))\n",
    "        for ind,entry in tqdm.tqdm(enumerate(series),desc=model_name + ' : ' + short_ctxts[i], unit_scale=True, total=len(series)):\n",
    "            inputs = token(ctx+\" \"+entry,return_tensors=\"pt\")\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            #If the entry is split in multiple tokens, we need to aggregate the tensors\n",
    "            expected_length = len(token([entry],is_split_into_words=True,add_special_tokens=False)['input_ids']) #number of tokens for the entry\n",
    "            last_ctx_token = token(ctx,add_special_tokens=False)['input_ids'][-1] #last token of the context string\n",
    "            start_index = inputs['input_ids'][0].tolist().index(last_ctx_token)+1 #first index of the entry's tensors\n",
    "\n",
    "            entry_embedding = outputs.last_hidden_state[0][start_index:start_index+expected_length].mean(axis=0)\n",
    "            buffer_arr[ind] = entry_embedding.detach().numpy()\n",
    "    \n",
    "        with open('embd_files/'+model_name+'_'+ short_ctxts[i] + '_cities.npy','wb') as f:\n",
    "            np.save(file=f,arr=buffer_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bert : come: 100%|██████████| 242/242 [00:18<00:00, 12.9it/s] \n",
      "bert : lives: 100%|██████████| 242/242 [00:16<00:00, 14.4it/s] \n",
      "bert : moved: 100%|██████████| 242/242 [00:18<00:00, 13.1it/s] \n",
      "bertLarge : come: 100%|██████████| 242/242 [00:57<00:00, 4.23it/s] \n",
      "bertLarge : lives: 100%|██████████| 242/242 [00:55<00:00, 4.36it/s] \n",
      "bertLarge : moved: 100%|██████████| 242/242 [00:55<00:00, 4.33it/s] \n",
      "gpt2 : come: 100%|██████████| 242/242 [00:17<00:00, 13.6it/s] \n",
      "gpt2 : lives: 100%|██████████| 242/242 [00:17<00:00, 13.9it/s] \n",
      "gpt2 : moved: 100%|██████████| 242/242 [00:17<00:00, 13.9it/s] \n",
      "roberta : come: 100%|██████████| 242/242 [00:17<00:00, 14.0it/s] \n",
      "roberta : lives: 100%|██████████| 242/242 [00:20<00:00, 12.0it/s] \n",
      "roberta : moved: 100%|██████████| 242/242 [00:17<00:00, 13.6it/s] \n",
      "mpnet : come: 100%|██████████| 242/242 [00:17<00:00, 14.1it/s] \n",
      "mpnet : lives: 100%|██████████| 242/242 [00:17<00:00, 14.2it/s] \n",
      "mpnet : moved: 100%|██████████| 242/242 [00:16<00:00, 14.4it/s] \n"
     ]
    }
   ],
   "source": [
    "# Capitals\n",
    "series = capitals.CapitalName\n",
    "for model_name in models_list:\n",
    "    token = tokenizers[model_name]\n",
    "    model = models[model_name]\n",
    "    size = models_output_size[model_name]\n",
    "    for i, ctx in enumerate(contexts):\n",
    "        buffer_arr = np.empty((len(series),size))\n",
    "        for ind,entry in tqdm.tqdm(enumerate(series),desc=model_name + ' : ' + short_ctxts[i], unit_scale=True, total=len(series)):\n",
    "            inputs = token(ctx+\" \"+entry,return_tensors=\"pt\")\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            #If the entry is split in multiple tokens, we need to aggregate the tensors\n",
    "            expected_length = len(token([entry],is_split_into_words=True,add_special_tokens=False)['input_ids']) #number of tokens for the entry\n",
    "            last_ctx_token = token(ctx,add_special_tokens=False)['input_ids'][-1] #last token of the context string\n",
    "            start_index = inputs['input_ids'][0].tolist().index(last_ctx_token)+1 #first index of the entry's tensors\n",
    "\n",
    "            entry_embedding = outputs.last_hidden_state[0][start_index:start_index+expected_length].mean(axis=0)\n",
    "            buffer_arr[ind] = entry_embedding.detach().numpy()\n",
    "    \n",
    "        with open('embd_files/'+model_name+'_'+ short_ctxts[i] + '_capitals.npy','wb') as f:\n",
    "            np.save(file=f,arr=buffer_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bert : come: 100%|██████████| 249/249 [00:18<00:00, 13.6it/s] \n",
      "bert : lives: 100%|██████████| 249/249 [00:18<00:00, 13.8it/s] \n",
      "bert : moved: 100%|██████████| 249/249 [00:17<00:00, 13.8it/s] \n",
      "bertLarge : come: 100%|██████████| 249/249 [00:57<00:00, 4.34it/s] \n",
      "bertLarge : lives: 100%|██████████| 249/249 [00:59<00:00, 4.18it/s] \n",
      "bertLarge : moved: 100%|██████████| 249/249 [00:56<00:00, 4.42it/s] \n",
      "gpt2 : come: 100%|██████████| 249/249 [00:18<00:00, 13.8it/s] \n",
      "gpt2 : lives: 100%|██████████| 249/249 [00:18<00:00, 13.8it/s] \n",
      "gpt2 : moved: 100%|██████████| 249/249 [00:18<00:00, 13.4it/s] \n",
      "roberta : come: 100%|██████████| 249/249 [00:17<00:00, 13.9it/s] \n",
      "roberta : lives: 100%|██████████| 249/249 [00:17<00:00, 14.1it/s] \n",
      "roberta : moved: 100%|██████████| 249/249 [00:17<00:00, 14.0it/s] \n",
      "mpnet : come: 100%|██████████| 249/249 [00:17<00:00, 14.4it/s] \n",
      "mpnet : lives: 100%|██████████| 249/249 [00:17<00:00, 14.1it/s] \n",
      "mpnet : moved: 100%|██████████| 249/249 [00:17<00:00, 14.2it/s] \n"
     ]
    }
   ],
   "source": [
    "# Countries\n",
    "series = countries.Name\n",
    "for model_name in models_list:\n",
    "    token = tokenizers[model_name]\n",
    "    model = models[model_name]\n",
    "    size = models_output_size[model_name]\n",
    "    for i, ctx in enumerate(contexts):\n",
    "        buffer_arr = np.empty((len(series),size))\n",
    "        for ind,entry in tqdm.tqdm(enumerate(series),desc=model_name + ' : ' + short_ctxts[i], unit_scale=True, total=len(series)):\n",
    "            inputs = token(ctx+\" \"+entry,return_tensors=\"pt\")\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            #If the entry is split in multiple tokens, we need to aggregate the tensors\n",
    "            expected_length = len(token([entry],is_split_into_words=True,add_special_tokens=False)['input_ids']) #number of tokens for the entry\n",
    "            last_ctx_token = token(ctx,add_special_tokens=False)['input_ids'][-1] #last token of the context string\n",
    "            start_index = inputs['input_ids'][0].tolist().index(last_ctx_token)+1 #first index of the entry's tensors\n",
    "\n",
    "            entry_embedding = outputs.last_hidden_state[0][start_index:start_index+expected_length].mean(axis=0)\n",
    "            buffer_arr[ind] = entry_embedding.detach().numpy()\n",
    "    \n",
    "        with open('embd_files/'+model_name+'_'+ short_ctxts[i] + '_countries.npy','wb') as f:\n",
    "            np.save(file=f,arr=buffer_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average of last 4 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bert : come: 100%|██████████| 3.53k/3.53k [04:07<00:00, 14.2it/s] \n",
      "bert : lives: 100%|██████████| 3.53k/3.53k [04:21<00:00, 13.5it/s]\n",
      "bert : moved: 100%|██████████| 3.53k/3.53k [04:17<00:00, 13.7it/s]\n",
      "bertLarge : come: 100%|██████████| 3.53k/3.53k [13:37<00:00, 4.31it/s] \n",
      "bertLarge : lives: 100%|██████████| 3.53k/3.53k [13:13<00:00, 4.44it/s]\n",
      "bertLarge : moved: 100%|██████████| 3.53k/3.53k [13:05<00:00, 4.49it/s]\n",
      "gpt2 : come: 100%|██████████| 3.53k/3.53k [03:55<00:00, 14.9it/s] \n",
      "gpt2 : lives: 100%|██████████| 3.53k/3.53k [03:56<00:00, 14.9it/s]\n",
      "gpt2 : moved: 100%|██████████| 3.53k/3.53k [03:53<00:00, 15.1it/s]\n",
      "roberta : come: 100%|██████████| 3.53k/3.53k [03:53<00:00, 15.1it/s] \n",
      "roberta : lives: 100%|██████████| 3.53k/3.53k [03:53<00:00, 15.1it/s]\n",
      "roberta : moved: 100%|██████████| 3.53k/3.53k [03:53<00:00, 15.1it/s]\n",
      "mpnet : come: 100%|██████████| 3.53k/3.53k [03:49<00:00, 15.3it/s] \n",
      "mpnet : lives: 100%|██████████| 3.53k/3.53k [03:49<00:00, 15.4it/s]\n",
      "mpnet : moved: 100%|██████████| 3.53k/3.53k [03:50<00:00, 15.3it/s]\n"
     ]
    }
   ],
   "source": [
    "# Cities\n",
    "series = cities.AccentCity\n",
    "for model_name in models_list:\n",
    "    token = tokenizers[model_name]\n",
    "    model = models[model_name]\n",
    "    size = models_output_size[model_name]\n",
    "    for i, ctx in enumerate(contexts):\n",
    "        buffer_arr = np.empty((len(series),size))\n",
    "        for ind,entry in tqdm.tqdm(enumerate(series),desc=model_name + ' : ' + short_ctxts[i], unit_scale=True, total=len(series)):\n",
    "            inputs = token(ctx+\" \"+entry,return_tensors=\"pt\")\n",
    "            outputs = model(**inputs,output_hidden_states=True)\n",
    "            \n",
    "            #If the entry is split in multiple tokens, we need to aggregate the tensors\n",
    "            expected_length = len(token([entry],is_split_into_words=True,add_special_tokens=False)['input_ids']) #number of tokens for the entry\n",
    "            last_ctx_token = token(ctx,add_special_tokens=False)['input_ids'][-1] #last token of the context string\n",
    "            start_index = inputs['input_ids'][0].tolist().index(last_ctx_token)+1 #first index of the entry's tensors\n",
    "\n",
    "            entry_embedding = torch.stack(outputs.hidden_states[-4:],axis=0)[:,0,:,:].mean(axis=0)[start_index:start_index+expected_length].mean(axis=0)\n",
    "            buffer_arr[ind] = entry_embedding.detach().numpy()\n",
    "    \n",
    "        with open('embd_files/4layers_'+model_name+'_'+ short_ctxts[i] + '_cities.npy','wb') as f:\n",
    "            np.save(file=f,arr=buffer_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bert : come: 100%|██████████| 242/242 [00:16<00:00, 14.6it/s] \n",
      "bert : lives: 100%|██████████| 242/242 [00:16<00:00, 14.9it/s] \n",
      "bert : moved: 100%|██████████| 242/242 [00:16<00:00, 14.8it/s] \n",
      "bertLarge : come: 100%|██████████| 242/242 [00:55<00:00, 4.38it/s] \n",
      "bertLarge : lives: 100%|██████████| 242/242 [00:52<00:00, 4.62it/s] \n",
      "bertLarge : moved: 100%|██████████| 242/242 [00:54<00:00, 4.44it/s] \n",
      "gpt2 : come: 100%|██████████| 242/242 [00:15<00:00, 15.2it/s] \n",
      "gpt2 : lives: 100%|██████████| 242/242 [00:16<00:00, 15.0it/s] \n",
      "gpt2 : moved: 100%|██████████| 242/242 [00:15<00:00, 15.5it/s] \n",
      "roberta : come: 100%|██████████| 242/242 [00:16<00:00, 14.9it/s] \n",
      "roberta : lives: 100%|██████████| 242/242 [00:15<00:00, 15.4it/s] \n",
      "roberta : moved: 100%|██████████| 242/242 [00:16<00:00, 14.9it/s] \n",
      "mpnet : come: 100%|██████████| 242/242 [00:15<00:00, 15.5it/s] \n",
      "mpnet : lives: 100%|██████████| 242/242 [00:15<00:00, 15.5it/s] \n",
      "mpnet : moved: 100%|██████████| 242/242 [00:15<00:00, 15.2it/s] \n"
     ]
    }
   ],
   "source": [
    "# Capitals\n",
    "series = capitals.CapitalName\n",
    "for model_name in models_list:\n",
    "    token = tokenizers[model_name]\n",
    "    model = models[model_name]\n",
    "    size = models_output_size[model_name]\n",
    "    for i, ctx in enumerate(contexts):\n",
    "        buffer_arr = np.empty((len(series),size))\n",
    "        for ind,entry in tqdm.tqdm(enumerate(series),desc=model_name + ' : ' + short_ctxts[i], unit_scale=True, total=len(series)):\n",
    "            inputs = token(ctx+\" \"+entry,return_tensors=\"pt\")\n",
    "            outputs = model(**inputs,output_hidden_states=True)\n",
    "            \n",
    "            #If the entry is split in multiple tokens, we need to aggregate the tensors\n",
    "            expected_length = len(token([entry],is_split_into_words=True,add_special_tokens=False)['input_ids']) #number of tokens for the entry\n",
    "            last_ctx_token = token(ctx,add_special_tokens=False)['input_ids'][-1] #last token of the context string\n",
    "            start_index = inputs['input_ids'][0].tolist().index(last_ctx_token)+1 #first index of the entry's tensors\n",
    "\n",
    "            entry_embedding = torch.stack(outputs.hidden_states[-4:],axis=0)[:,0,:,:].mean(axis=0)[start_index:start_index+expected_length].mean(axis=0)\n",
    "            buffer_arr[ind] = entry_embedding.detach().numpy()\n",
    "    \n",
    "        with open('embd_files/4layers_'+model_name+'_'+ short_ctxts[i] + '_capitals.npy','wb') as f:\n",
    "            np.save(file=f,arr=buffer_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bert : come: 100%|██████████| 249/249 [00:16<00:00, 15.3it/s] \n",
      "bert : lives: 100%|██████████| 249/249 [00:15<00:00, 16.0it/s] \n",
      "bert : moved: 100%|██████████| 249/249 [00:15<00:00, 15.9it/s] \n",
      "bertLarge : come: 100%|██████████| 249/249 [00:53<00:00, 4.63it/s] \n",
      "bertLarge : lives: 100%|██████████| 249/249 [00:50<00:00, 4.89it/s] \n",
      "bertLarge : moved: 100%|██████████| 249/249 [00:49<00:00, 5.02it/s] \n",
      "gpt2 : come: 100%|██████████| 249/249 [00:17<00:00, 14.0it/s] \n",
      "gpt2 : lives: 100%|██████████| 249/249 [00:15<00:00, 16.5it/s] \n",
      "gpt2 : moved: 100%|██████████| 249/249 [00:15<00:00, 16.1it/s] \n",
      "roberta : come: 100%|██████████| 249/249 [00:15<00:00, 15.9it/s] \n",
      "roberta : lives: 100%|██████████| 249/249 [00:15<00:00, 16.2it/s] \n",
      "roberta : moved: 100%|██████████| 249/249 [00:15<00:00, 16.6it/s] \n",
      "mpnet : come: 100%|██████████| 249/249 [00:17<00:00, 14.5it/s] \n",
      "mpnet : lives: 100%|██████████| 249/249 [00:16<00:00, 15.5it/s] \n",
      "mpnet : moved: 100%|██████████| 249/249 [00:16<00:00, 15.0it/s] \n"
     ]
    }
   ],
   "source": [
    "# Countries\n",
    "series = countries.Name\n",
    "for model_name in models_list:\n",
    "    token = tokenizers[model_name]\n",
    "    model = models[model_name]\n",
    "    size = models_output_size[model_name]\n",
    "    for i, ctx in enumerate(contexts):\n",
    "        buffer_arr = np.empty((len(series),size))\n",
    "        for ind,entry in tqdm.tqdm(enumerate(series),desc=model_name + ' : ' + short_ctxts[i], unit_scale=True, total=len(series)):\n",
    "            inputs = token(ctx+\" \"+entry,return_tensors=\"pt\")\n",
    "            outputs = model(**inputs,output_hidden_states=True)\n",
    "            \n",
    "            #If the entry is split in multiple tokens, we need to aggregate the tensors\n",
    "            expected_length = len(token([entry],is_split_into_words=True,add_special_tokens=False)['input_ids']) #number of tokens for the entry\n",
    "            last_ctx_token = token(ctx,add_special_tokens=False)['input_ids'][-1] #last token of the context string\n",
    "            start_index = inputs['input_ids'][0].tolist().index(last_ctx_token)+1 #first index of the entry's tensors\n",
    "\n",
    "            entry_embedding = torch.stack(outputs.hidden_states[-4:],axis=0)[:,0,:,:].mean(axis=0)[start_index:start_index+expected_length].mean(axis=0)\n",
    "            buffer_arr[ind] = entry_embedding.detach().numpy()\n",
    "    \n",
    "        with open('embd_files/4layers_'+model_name+'_'+ short_ctxts[i] + '_countries.npy','wb') as f:\n",
    "            np.save(file=f,arr=buffer_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
